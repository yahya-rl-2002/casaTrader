#!/usr/bin/env python3
"""
Script de test pour v√©rifier que le scraper am√©lior√© peut scraper
BourseNews, Medias24 et Hespress
"""
import asyncio
import sys
from pathlib import Path

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent))

from app.pipelines.ingestion.enhanced_media_scraper import EnhancedMediaScraper
from app.core.logging import get_logger

logger = get_logger(__name__)


async def test_scraper():
    """Tester le scraper am√©lior√© avec les 3 sources principales"""
    
    print("=" * 80)
    print("üß™ TEST DU SCRAPER AM√âLIOR√â")
    print("=" * 80)
    print()
    
    scraper = EnhancedMediaScraper(
        delay_between_requests=2.0,
        max_retries=2,  # R√©duire pour les tests
        min_content_length=200,  # R√©duire pour les tests
        max_article_age_days=7
    )
    
    # Sources √† tester
    test_sources = {
        "medias24": [
            "https://medias24.com",
            "https://medias24.com/economie/",
        ],
        "boursenews": [
            "https://boursenews.ma",
            "https://boursenews.ma/espace-investisseurs",
        ],
        "hespress": [
            "https://fr.hespress.com/economie",
        ],
    }
    
    results = {}
    
    for source_name, listing_urls in test_sources.items():
        print(f"\n{'=' * 80}")
        print(f"üì∞ TEST: {source_name.upper()}")
        print(f"{'=' * 80}")
        
        source_articles = []
        
        for listing_url in listing_urls:
            print(f"\nüîç Scraping listing: {listing_url}")
            
            try:
                # Scraper les articles depuis le listing
                articles = await asyncio.to_thread(
                    scraper.scrape_articles_from_listing,
                    listing_url,
                    source_name,
                    max_articles=5  # Limiter √† 5 pour les tests
                )
                
                source_articles.extend(articles)
                
                print(f"‚úÖ {len(articles)} articles trouv√©s depuis {listing_url}")
                
                # Afficher les d√©tails des articles
                for i, article in enumerate(articles[:3], 1):  # Afficher les 3 premiers
                    print(f"\n  Article {i}:")
                    print(f"    Titre: {article.title[:60]}...")
                    print(f"    URL: {article.url}")
                    print(f"    Contenu: {len(article.content)} caract√®res")
                    print(f"    Qualit√©: {article.quality_score:.2f}")
                    print(f"    Mots: {article.word_count}")
                    if article.published_at:
                        print(f"    Date: {article.published_at}")
                
            except Exception as e:
                print(f"‚ùå Erreur scraping {listing_url}: {e}")
                logger.error(f"Erreur scraping {listing_url}: {e}", exc_info=True)
                continue
        
        results[source_name] = {
            "total": len(source_articles),
            "articles": source_articles
        }
        
        print(f"\nüìä R√©sultat {source_name}: {len(source_articles)} articles scrap√©s")
    
    # R√©sum√© final
    print(f"\n{'=' * 80}")
    print("üìä R√âSUM√â DES TESTS")
    print(f"{'=' * 80}")
    
    total_articles = sum(r["total"] for r in results.values())
    
    for source_name, result in results.items():
        status = "‚úÖ" if result["total"] > 0 else "‚ùå"
        print(f"{status} {source_name}: {result['total']} articles")
    
    print(f"\nüéØ Total: {total_articles} articles scrap√©s")
    
    if total_articles > 0:
        print("\n‚úÖ SUCC√àS: Le scraper peut scraper tous les m√©dias !")
    else:
        print("\n‚ùå √âCHEC: Aucun article n'a √©t√© scrap√©")
    
    return results


if __name__ == "__main__":
    try:
        results = asyncio.run(test_scraper())
        
        # Afficher quelques exemples d'articles
        print(f"\n{'=' * 80}")
        print("üìÑ EXEMPLES D'ARTICLES SCRAP√âS")
        print(f"{'=' * 80}")
        
        for source_name, result in results.items():
            if result["articles"]:
                article = result["articles"][0]
                print(f"\n{source_name.upper()}:")
                print(f"  Titre: {article.title}")
                print(f"  URL: {article.url}")
                print(f"  Contenu (premiers 200 caract√®res):")
                print(f"  {article.content[:200]}...")
                print(f"  Qualit√©: {article.quality_score:.2f}")
                print()
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Test interrompu par l'utilisateur")
    except Exception as e:
        print(f"\n\n‚ùå Erreur lors du test: {e}")
        logger.error(f"Erreur lors du test: {e}", exc_info=True)
        sys.exit(1)






