#!/usr/bin/env python3
"""
Analyse de BourseNews.ma et ses protections anti-scraping
"""
import sys
sys.path.append('.')

import requests
from bs4 import BeautifulSoup
import time
import json


def test_basic_request():
    """Test requÃªte basique"""
    print("ğŸ” Test #1 - RequÃªte Basique")
    print("=" * 80)
    
    session = requests.Session()
    session.verify = False
    
    import urllib3
    urllib3.disable_warnings()
    
    url = "https://www.boursenews.ma"
    
    print(f"URL: {url}")
    try:
        response = session.get(url, timeout=10)
        print(f"âœ… Status Code: {response.status_code}")
        print(f"   Content-Length: {len(response.content)} bytes")
        print(f"   Content-Type: {response.headers.get('Content-Type', 'N/A')}")
        
        # VÃ©rifier si c'est du JavaScript/Cloudflare/etc
        if 'cf-ray' in response.headers:
            print("   âš ï¸  Cloudflare dÃ©tectÃ©!")
        if 'server' in response.headers:
            print(f"   Server: {response.headers['server']}")
        
        # Analyser le contenu
        if 'javascript' in response.text.lower()[:500]:
            print("   âš ï¸  Protection JavaScript dÃ©tectÃ©e dans les premiÃ¨res lignes")
        if 'captcha' in response.text.lower():
            print("   âš ï¸  CAPTCHA possible")
        if 'challenge' in response.text.lower()[:1000]:
            print("   âš ï¸  Challenge dÃ©tectÃ© (anti-bot)")
        
        return response
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        return None


def test_with_headers():
    """Test avec headers avancÃ©s"""
    print("\n\nğŸ” Test #2 - RequÃªte avec Headers AvancÃ©s")
    print("=" * 80)
    
    session = requests.Session()
    session.verify = False
    
    # Headers plus rÃ©alistes
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'Referer': 'https://www.google.com/'
    }
    session.headers.update(headers)
    
    import urllib3
    urllib3.disable_warnings()
    
    url = "https://www.boursenews.ma"
    
    try:
        response = session.get(url, timeout=15)
        print(f"âœ… Status Code: {response.status_code}")
        print(f"   Content-Length: {len(response.content)} bytes")
        
        # Sauvegarder pour inspection
        with open('boursenews_response.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"   ğŸ’¾ SauvegardÃ© dans boursenews_response.html")
        
        return response
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        return None


def test_selenium():
    """Test avec Selenium (navigateur rÃ©el)"""
    print("\n\nğŸ” Test #3 - Selenium (Navigateur RÃ©el)")
    print("=" * 80)
    
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        
        print("âœ… Selenium disponible")
        
        # Configuration
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # Mode sans interface
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
        
        print("ğŸš€ Lancement du navigateur...")
        driver = webdriver.Chrome(options=chrome_options)
        
        url = "https://www.boursenews.ma"
        print(f"ğŸŒ Navigation vers {url}")
        driver.get(url)
        
        # Attendre le chargement
        time.sleep(3)
        
        print(f"âœ… Page chargÃ©e")
        print(f"   Titre: {driver.title}")
        
        # Chercher des articles
        articles = driver.find_elements(By.TAG_NAME, 'article')
        print(f"   Articles trouvÃ©s: {len(articles)}")
        
        # Sauvegarder le HTML
        with open('boursenews_selenium.html', 'w', encoding='utf-8') as f:
            f.write(driver.page_source)
        print(f"   ğŸ’¾ HTML sauvegardÃ© dans boursenews_selenium.html")
        
        driver.quit()
        return True
        
    except ImportError:
        print("âš ï¸  Selenium non installÃ©")
        print("   Installation: pip install selenium")
        return False
    except Exception as e:
        print(f"âŒ Erreur Selenium: {e}")
        return False


def analyze_structure(response):
    """Analyser la structure du site"""
    print("\n\nğŸ” Test #4 - Analyse Structure HTML")
    print("=" * 80)
    
    if not response:
        print("âŒ Pas de rÃ©ponse Ã  analyser")
        return
    
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Chercher articles
    print("\nğŸ“° Recherche d'articles...")
    
    selectors = [
        ('article', 'Tag article'),
        ('.post', 'Classe .post'),
        ('.entry', 'Classe .entry'),
        ('.article-item', 'Classe .article-item'),
        ('.news-item', 'Classe .news-item'),
        ('[class*="article"]', 'Contient "article"'),
        ('[class*="post"]', 'Contient "post"'),
    ]
    
    for selector, desc in selectors:
        elements = soup.select(selector)
        print(f"   {desc:30} : {len(elements)} Ã©lÃ©ments")
        
        if elements and len(elements) > 0:
            print(f"      â†’ Premier Ã©lÃ©ment: {str(elements[0])[:100]}...")
    
    # Chercher des liens
    print("\nğŸ”— Analyse des liens...")
    links = soup.find_all('a', href=True)
    print(f"   Total liens: {len(links)}")
    
    # Filtrer liens articles
    article_links = [
        link for link in links 
        if 'article' in link['href'] or 'post' in link['href'] or len(link['href']) > 30
    ]
    print(f"   Liens articles potentiels: {len(article_links)}")
    
    if article_links:
        print(f"\n   Exemples:")
        for link in article_links[:5]:
            print(f"      â€¢ {link.get_text(strip=True)[:50]}...")
            print(f"        {link['href'][:70]}")


def test_api():
    """Chercher une API cachÃ©e"""
    print("\n\nğŸ” Test #5 - Recherche API")
    print("=" * 80)
    
    session = requests.Session()
    session.verify = False
    
    import urllib3
    urllib3.disable_warnings()
    
    # URLs API possibles
    api_urls = [
        "https://www.boursenews.ma/wp-json/wp/v2/posts",
        "https://www.boursenews.ma/api/articles",
        "https://www.boursenews.ma/feed",
        "https://www.boursenews.ma/feed/rss",
    ]
    
    for url in api_urls:
        print(f"\nğŸŒ Test: {url}")
        try:
            response = session.get(url, timeout=10)
            print(f"   Status: {response.status_code}")
            
            if response.status_code == 200:
                print(f"   âœ… API trouvÃ©e!")
                print(f"   Content-Type: {response.headers.get('Content-Type', 'N/A')}")
                
                if 'json' in response.headers.get('Content-Type', ''):
                    try:
                        data = response.json()
                        print(f"   ğŸ“Š JSON valide: {len(data)} entrÃ©es")
                        
                        # Sauvegarder
                        with open('boursenews_api.json', 'w', encoding='utf-8') as f:
                            json.dump(data, f, indent=2, ensure_ascii=False)
                        print(f"   ğŸ’¾ SauvegardÃ© dans boursenews_api.json")
                    except:
                        print(f"   âš ï¸  Pas du JSON valide")
                
                elif 'xml' in response.headers.get('Content-Type', ''):
                    print(f"   ğŸ“° RSS Feed trouvÃ©!")
                    with open('boursenews_rss.xml', 'w', encoding='utf-8') as f:
                        f.write(response.text)
                    print(f"   ğŸ’¾ SauvegardÃ© dans boursenews_rss.xml")
        
        except Exception as e:
            print(f"   âŒ Erreur: {type(e).__name__}")


def main():
    """Fonction principale"""
    print("\nğŸ•µï¸  ANALYSE DE BOURSENEWS.MA - PROTECTIONS ANTI-SCRAPING")
    print("=" * 80)
    print("Ce script analyse les protections et trouve les meilleures mÃ©thodes")
    print("=" * 80)
    
    # Test 1: RequÃªte basique
    response1 = test_basic_request()
    
    # Test 2: Headers avancÃ©s
    response2 = test_with_headers()
    
    # Test 3: Selenium
    test_selenium()
    
    # Test 4: Analyse structure
    if response2:
        analyze_structure(response2)
    
    # Test 5: API
    test_api()
    
    print("\n\nâœ… Analyse TerminÃ©e!")
    print("=" * 80)
    print("\nğŸ’¡ Recommandations:")
    print("   1. VÃ©rifier les fichiers sauvegardÃ©s (.html, .json, .xml)")
    print("   2. Si Cloudflare: Utiliser cloudscraper ou Selenium")
    print("   3. Si API/RSS: Utiliser directement l'API")
    print("   4. Respecter les dÃ©lais entre requÃªtes (2-5 secondes)")
    print("   5. Rotation User-Agent si nÃ©cessaire")


if __name__ == "__main__":
    main()








