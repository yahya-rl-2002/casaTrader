from datetime import datetime
from typing import List, Optional
import asyncio

from fastapi import APIRouter, Depends, Query, BackgroundTasks
from sqlalchemy import desc
from sqlalchemy.orm import Session

from app.models.schemas import MediaArticle
from app.core.logging import get_logger
from app.api.dependencies import get_db
from app.services.cache_service import get_cache_service

router = APIRouter()
logger = get_logger(__name__)

# Service de cache Redis (avec fallback en m√©moire)
cache_service = get_cache_service()


@router.get("/latest", summary="Latest media articles with sentiment")
async def get_latest_media(
    limit: int = Query(20, ge=1, le=500, description="Number of articles to return"),
    offset: int = Query(0, ge=0, description="Offset for pagination"),
    cursor: Optional[str] = Query(None, description="Cursor for cursor-based pagination (article ID)"),
    auto_scrape: bool = Query(False, description="Auto-scrape if articles are old"),
    db: Session = Depends(get_db),
    background_tasks: BackgroundTasks = BackgroundTasks()
) -> dict:
    """
    R√©cup√®re les derniers articles m√©dias avec leur analyse de sentiment.
    D√©clenche automatiquement le scraping si les articles sont anciens.
    
    - **limit**: Nombre d'articles √† retourner (1-500, d√©faut: 20)
    - **offset**: Offset pour pagination classique (d√©faut: 0)
    - **cursor**: ID de l'article pour pagination cursor-based (plus performant)
    - **auto_scrape**: D√©clencher automatiquement le scraping si n√©cessaire (d√©faut: False)
    
    **Optimisation** : Utilise cursor-based pagination si `cursor` est fourni (plus performant pour grandes listes).
    Sinon, utilise offset-based pagination classique.
    """
    try:
        # V√©rifier le cache pour les requ√™tes fr√©quentes
        cache_key = f"media:latest:{limit}:{offset}:{cursor or 'none'}"
        cached_result = cache_service.get(cache_key)
        if cached_result is not None:
            logger.debug(f"‚úÖ Cache hit: media articles (limit={limit}, offset={offset})")
            return cached_result
        # V√©rifier si la table existe, sinon la cr√©er
        from app.models.schemas import Base
        from app.models.database import engine
        Base.metadata.create_all(bind=engine, checkfirst=True)
        
        # V√©rifier si on a besoin de scraper
        if auto_scrape:
            from datetime import timedelta
            recent_count = db.query(MediaArticle).filter(
                MediaArticle.scraped_at >= datetime.now() - timedelta(hours=1)
            ).count()
            
            # Si moins de 10 articles r√©cents, d√©clencher le scraping en arri√®re-plan
            if recent_count < 10:
                logger.info(f"üîÑ D√©clenchement automatique du scraping (seulement {recent_count} articles r√©cents)")
                if background_tasks:
                    background_tasks.add_task(trigger_enhanced_scraping)
                else:
                    # Si pas de background_tasks, lancer en arri√®re-plan
                    asyncio.create_task(trigger_enhanced_scraping())
        
        # Pagination optimis√©e : cursor-based si cursor fourni, sinon offset-based
        if cursor:
            # Cursor-based pagination (plus performant)
            try:
                cursor_id = int(cursor)
                query = (
                    db.query(MediaArticle)
                    .filter(MediaArticle.id < cursor_id)  # Articles plus anciens que le cursor
                    .order_by(desc(MediaArticle.published_at))
                    .limit(limit + 1)  # +1 pour savoir s'il y a une page suivante
                )
            except ValueError:
                # Cursor invalide, fallback sur offset
                query = (
                    db.query(MediaArticle)
                    .order_by(desc(MediaArticle.published_at))
                    .offset(offset)
                    .limit(limit)
                )
                cursor = None
        else:
            # Offset-based pagination classique
            query = (
                db.query(MediaArticle)
                .order_by(desc(MediaArticle.published_at))
                .offset(offset)
                .limit(limit)
            )
        
        articles = query.all()
        
        # Pour cursor-based, v√©rifier s'il y a une page suivante
        has_next = False
        next_cursor = None
        if cursor and len(articles) > limit:
            has_next = True
            articles = articles[:limit]
            next_cursor = str(articles[-1].id) if articles else None
        
        result = {
            "data": [
                {
                    "id": article.id,
                    "source": article.source or "Source inconnue",
                    "title": article.title or "Titre non disponible",
                    "url": article.url or "#",
                    "summary": getattr(article, 'summary', None),
                    "content": getattr(article, 'content', None),  # Contenu complet
                    "image_url": getattr(article, 'image_url', None),  # URL de l'image principale
                    "published_at": article.published_at.isoformat() if article.published_at else None,
                    "sentiment_score": article.sentiment_score,
                    "sentiment_label": article.sentiment_label,
                    "scraped_at": article.scraped_at.isoformat() if article.scraped_at else None,
                }
                for article in articles
            ],
            "count": len(articles),
            "limit": limit,
        }
        
        # Ajouter les m√©tadonn√©es de pagination
        if cursor:
            result["pagination"] = {
                "type": "cursor",
                "has_next": has_next,
                "next_cursor": next_cursor,
            }
        else:
            result["pagination"] = {
                "type": "offset",
                "offset": offset,
                "has_next": len(articles) == limit,  # Probablement plus de r√©sultats
            }
        
        # Mettre en cache (TTL court pour les donn√©es dynamiques)
        cache_service.set(cache_key, result, ttl_seconds=60)  # 1 minute pour les articles
        
        return result
    except Exception as e:
        logger.error(f"Error fetching media articles: {e}", exc_info=True)
        
        # Retourner une structure vide en cas d'erreur
        return {
            "data": [],
            "count": 0,
            "limit": limit,
            "error": str(e)
        }


@router.get("/sources", summary="List of media sources")
async def get_media_sources(db: Session = Depends(get_db)) -> dict:
    """
    R√©cup√®re la liste des sources m√©dias avec le nombre d'articles par source.
    """
    from sqlalchemy import func
    
    sources = (
        db.query(
            MediaArticle.source,
            func.count(MediaArticle.id).label("count")
        )
        .group_by(MediaArticle.source)
        .all()
    )
    
    return {
        "sources": [
            {"name": source, "article_count": count}
            for source, count in sources
        ],
        "total_sources": len(sources),
    }


@router.get("/sentiment-stats", summary="Sentiment statistics")
async def get_sentiment_stats(db: Session = Depends(get_db)) -> dict:
    """
    R√©cup√®re des statistiques sur le sentiment des articles.
    """
    from sqlalchemy import func
    
    # Compter par label de sentiment
    sentiment_counts = (
        db.query(
            MediaArticle.sentiment_label,
            func.count(MediaArticle.id).label("count")
        )
        .filter(MediaArticle.sentiment_label.isnot(None))
        .group_by(MediaArticle.sentiment_label)
        .all()
    )
    
    # Statistiques sur les scores
    sentiment_stats = db.query(
        func.avg(MediaArticle.sentiment_score).label("avg"),
        func.min(MediaArticle.sentiment_score).label("min"),
        func.max(MediaArticle.sentiment_score).label("max"),
    ).filter(MediaArticle.sentiment_score.isnot(None)).first()
    
    total_articles = db.query(func.count(MediaArticle.id)).scalar()
    
    return {
        "total_articles": total_articles,
        "sentiment_distribution": {
            label: count for label, count in sentiment_counts
        },
        "sentiment_scores": {
            "average": float(sentiment_stats.avg) if sentiment_stats.avg else None,
            "min": float(sentiment_stats.min) if sentiment_stats.min else None,
            "max": float(sentiment_stats.max) if sentiment_stats.max else None,
        },
    }


@router.post("/trigger-scraping", summary="Trigger enhanced scraping")
async def trigger_scraping_endpoint(background_tasks: BackgroundTasks = BackgroundTasks()) -> dict:
    """
    D√©clencher manuellement le scraping am√©lior√© des articles.
    Le scraping s'ex√©cute en arri√®re-plan pour ne pas bloquer la requ√™te.
    """
    try:
        if background_tasks:
            background_tasks.add_task(trigger_enhanced_scraping)
        else:
            asyncio.create_task(trigger_enhanced_scraping())
        
        return {
            "message": "Scraping d√©clench√© en arri√®re-plan",
            "status": "running"
        }
    except Exception as e:
        logger.error(f"Erreur d√©clenchement scraping: {e}")
        return {
            "error": str(e),
            "status": "error"
        }


@router.post("/sync-to-supabase", summary="Sync articles to Supabase")
async def sync_to_supabase_endpoint(
    sources: List[str] = Query(None, description="Sources to sync (default: all configured sources)"),
    limit: int = Query(None, description="Limit number of articles to sync (default: all)")
) -> dict:
    """
    Synchroniser les articles de SQLite vers Supabase.
    Utile pour synchroniser imm√©diatement tous les articles existants.
    
    - **sources**: Liste des sources √† synchroniser (ex: ["hespress", "medias24", "boursenews"])
    - **limit**: Nombre maximum d'articles √† synchroniser (None = tous)
    """
    try:
        from app.services.supabase_sync_service import SupabaseSyncService
        
        sync_service = SupabaseSyncService()
        
        if not sync_service.client:
            return {
                "success": False,
                "error": "Client Supabase non initialis√©. V√©rifiez SUPABASE_URL et SUPABASE_ANON_KEY",
                "synced": 0
            }
        
        # Utiliser les sources configur√©es si non sp√©cifi√©es
        if not sources:
            sources = ["hespress", "medias24", "boursenews"]
        
        logger.info(f"üîÑ Synchronisation vers Supabase: {sources}")
        
        stats = sync_service.sync_articles_to_supabase(
            sources=sources,
            limit=limit
        )
        
        return {
            "success": stats.get("success", False),
            "synced": stats.get("synced", 0),
            "sources": stats.get("sources", {}),
            "errors": stats.get("errors", []),
            "message": f"{stats.get('synced', 0)} articles synchronis√©s vers Supabase"
        }
        
    except ImportError as e:
        logger.error(f"Erreur import SupabaseSyncService: {e}")
        return {
            "success": False,
            "error": "Service de synchronisation non disponible. Installez supabase: pip install supabase",
            "synced": 0
        }
    except Exception as e:
        logger.error(f"Erreur synchronisation Supabase: {e}", exc_info=True)
        return {
            "success": False,
            "error": str(e),
            "synced": 0
        }


@router.get("/cache/stats", summary="Get cache statistics")
async def get_cache_stats() -> dict:
    """
    Retourne les statistiques du cache (Redis ou m√©moire).
    Utile pour le monitoring et le debugging.
    """
    stats = cache_service.get_stats()
    return {
        "cache": stats,
        "message": "Cache statistics retrieved successfully"
    }


@router.delete("/cache/clear", summary="Clear cache")
async def clear_cache(pattern: Optional[str] = Query(None, description="Pattern to clear (ex: 'volume:*')")) -> dict:
    """
    Vide le cache. Si un pattern est fourni, supprime seulement les cl√©s correspondantes.
    
    - **pattern**: Pattern Redis (ex: "volume:*", "simplified:*")
    """
    if pattern:
        count = cache_service.delete_pattern(pattern)
        return {
            "success": True,
            "message": f"Cleared {count} cache keys matching pattern '{pattern}'"
        }
    else:
        success = cache_service.clear()
        return {
            "success": success,
            "message": "Cache cleared successfully" if success else "Failed to clear cache"
        }


async def trigger_enhanced_scraping():
    """
    Fonction pour d√©clencher le scraping am√©lior√© en arri√®re-plan
    """
    try:
        from app.services.enhanced_media_service import EnhancedMediaService
        
        logger.info("üöÄ D√©marrage du scraping am√©lior√© en arri√®re-plan")
        service = EnhancedMediaService()
        result = await service.trigger_scraping_on_access()
        
        logger.info(f"‚úÖ Scraping termin√©: {result}")
        
        # Synchroniser vers Supabase apr√®s le scraping
        try:
            from app.services.supabase_sync_service import SupabaseSyncService
            sync_service = SupabaseSyncService()
            if sync_service.client:
                logger.info("üîÑ Synchronisation vers Supabase...")
                sync_stats = sync_service.sync_recent_articles(hours=24)
                if sync_stats.get("success"):
                    logger.info(f"‚úÖ {sync_stats['synced']} articles synchronis√©s vers Supabase")
                else:
                    logger.warning(f"‚ö†Ô∏è  Erreur synchronisation Supabase: {sync_stats.get('error')}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Impossible de synchroniser vers Supabase: {e}")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du scraping am√©lior√©: {e}", exc_info=True)

