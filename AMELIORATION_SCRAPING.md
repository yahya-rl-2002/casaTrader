# üöÄ Am√©lioration du Syst√®me de Scraping

**Date**: Aujourd'hui  
**Objectif**: Am√©liorer la qualit√© du scraping des articles pour que les utilisateurs trouvent des articles d√©j√† scrap√©s en temps r√©el avec un contenu complet et de qualit√©.

---

## üìã Probl√®mes Identifi√©s

### ‚ùå Probl√®mes Actuels

1. **Contenu incomplet** : Les scrapers ne r√©cup√®rent que le titre et le r√©sum√© depuis la page de listing, pas le contenu complet de l'article
2. **Pas de scraping individuel** : Les articles ne sont pas scrap√©s individuellement pour obtenir le contenu complet
3. **Pas de validation de qualit√©** : Aucune validation robuste de la qualit√© des articles
4. **Pas de retry** : Si un article √©choue, il est simplement ignor√©
5. **Pas de v√©rification de fra√Æcheur** : Les articles peuvent √™tre anciens
6. **Pas de scraping en temps r√©el** : Le scraping n'est pas d√©clench√© automatiquement quand l'utilisateur acc√®de

---

## ‚úÖ Am√©liorations Impl√©ment√©es

### 1. **Scraper Am√©lior√© avec Contenu Complet** (`enhanced_media_scraper.py`)

#### Fonctionnalit√©s

- ‚úÖ **Scraping individuel** : Chaque article est scrap√© individuellement pour obtenir son contenu complet
- ‚úÖ **Extraction multi-m√©thodes** : Utilise plusieurs m√©thodes de fallback pour extraire le contenu :
  - Balises `<article>` standard
  - Divs avec classes communes (`.article-content`, `.post-content`, etc.)
  - Paragraphes les plus longs
  - Texte principal du body
- ‚úÖ **M√©tadonn√©es enrichies** : R√©cup√®re titre, description, image, auteur, cat√©gorie, tags, date
- ‚úÖ **Validation de qualit√©** : Score de qualit√© (0-1) bas√© sur :
  - Longueur du contenu (40%)
  - Pr√©sence de mots-cl√©s financiers (30%)
  - M√©tadonn√©es compl√®tes (20%)
  - Fra√Æcheur de l'article (10%)
- ‚úÖ **Syst√®me de retry** : Retry avec backoff exponentiel (2^attempt secondes)
- ‚úÖ **Cache intelligent** : √âvite de re-scraper les m√™mes articles (cache 24h)
- ‚úÖ **V√©rification de fra√Æcheur** : Filtre les articles trop anciens (max 7 jours par d√©faut)
- ‚úÖ **Rotation User-Agent** : √âvite la d√©tection anti-bot

### 2. **Service Am√©lior√©** (`enhanced_media_service.py`)

#### Fonctionnalit√©s

- ‚úÖ **Scraping multi-sources** : Scrape toutes les sources (Medias24, BourseNews, Challenge, La Vie √âco, L'√âconomiste)
- ‚úÖ **Scraping optimis√©** : Version rapide pour scraping en temps r√©el
- ‚úÖ **D√©tection automatique** : V√©rifie si des articles r√©cents existent avant de scraper
- ‚úÖ **Sauvegarde intelligente** : √âvite les doublons par URL, met √† jour si meilleure qualit√©
- ‚úÖ **Statistiques d√©taill√©es** : Retourne des stats par source (nombre scrap√©, qualit√© moyenne, etc.)

### 3. **Endpoint API Am√©lior√©** (`media.py`)

#### Nouveaux Endpoints

- ‚úÖ **GET `/api/v1/media/latest`** : 
  - D√©clenche automatiquement le scraping si articles anciens
  - Retourne le contenu complet des articles
  - Param√®tre `auto_scrape=true` (par d√©faut)
  
- ‚úÖ **POST `/api/v1/media/trigger-scraping`** :
  - D√©clenche manuellement le scraping am√©lior√©
  - S'ex√©cute en arri√®re-plan

#### Am√©liorations

- ‚úÖ **D√©clenchement automatique** : Quand l'utilisateur acc√®de √† `/api/v1/media/latest`, le syst√®me v√©rifie si des articles r√©cents existent (< 1h). Si moins de 10 articles r√©cents, d√©clenche le scraping en arri√®re-plan
- ‚úÖ **Contenu complet** : L'endpoint retourne maintenant le champ `content` avec le contenu complet de l'article

---

## üéØ Workflow Utilisateur

### Sc√©nario 1 : Acc√®s √† la Page Actualit√©s

1. **Utilisateur acc√®de** √† la page Actualit√©s
2. **Frontend appelle** `GET /api/v1/media/latest?limit=50&auto_scrape=true`
3. **Backend v√©rifie** :
   - Si articles r√©cents (< 1h) >= 10 ‚Üí Retourne les articles
   - Si articles r√©cents < 10 ‚Üí D√©clenche le scraping en arri√®re-plan ET retourne les articles existants
4. **Scraping en arri√®re-plan** :
   - R√©cup√®re les liens depuis les pages de listing
   - Scrape le contenu complet de chaque article individuellement
   - Valide la qualit√© (score >= 0.3)
   - Sauvegarde en base de donn√©es
5. **Frontend affiche** les articles imm√©diatement (m√™me s'ils sont anciens)
6. **Quand le scraping termine** : Les nouveaux articles apparaissent automatiquement (si le frontend fait un refresh)

### Sc√©nario 2 : Actualisation Manuelle

1. **Utilisateur clique** sur "Actualiser"
2. **Frontend appelle** `POST /api/v1/media/trigger-scraping`
3. **Backend d√©clenche** le scraping am√©lior√© en arri√®re-plan
4. **Retour imm√©diat** : `{"status": "running", "message": "Scraping d√©clench√©"}`
5. **Scraping s'ex√©cute** en arri√®re-plan
6. **Frontend peut** :
   - Afficher un indicateur de chargement
   - Poller `/api/v1/media/latest` pour voir les nouveaux articles
   - Ou attendre quelques secondes puis recharger

---

## üìä Am√©liorations de Qualit√©

### Avant

- ‚ùå Contenu : Seulement titre + r√©sum√© (50-200 caract√®res)
- ‚ùå Qualit√© : Pas de validation
- ‚ùå Fra√Æcheur : Articles peuvent √™tre anciens
- ‚ùå Doublons : Peu de d√©tection
- ‚ùå Retry : Aucun retry en cas d'√©chec

### Apr√®s

- ‚úÖ Contenu : Titre + r√©sum√© + **contenu complet** (300+ caract√®res, souvent 500-2000)
- ‚úÖ Qualit√© : Score de qualit√© (0-1) avec validation robuste
- ‚úÖ Fra√Æcheur : Filtre les articles > 7 jours
- ‚úÖ Doublons : D√©tection par URL, mise √† jour si meilleure qualit√©
- ‚úÖ Retry : Retry avec backoff exponentiel (3 tentatives)

---

## üîß Configuration

### Param√®tres du Scraper

```python
EnhancedMediaScraper(
    delay_between_requests=2.0,      # D√©lai entre requ√™tes (secondes)
    max_retries=3,                    # Nombre de tentatives
    cache_dir="cache/scraping",       # R√©pertoire de cache
    min_content_length=300,            # Longueur minimale du contenu
    max_article_age_days=7            # √Çge maximum des articles
)
```

### Param√®tres du Service

```python
EnhancedMediaService(
    cache_dir="cache/scraping",       # R√©pertoire de cache
    max_articles_per_source=15,       # Nombre max d'articles par source
    min_quality_score=0.3             # Score de qualit√© minimum (0-1)
)
```

---

## üìù Fichiers Cr√©√©s/Modifi√©s

### Nouveaux Fichiers

1. **`backend/app/pipelines/ingestion/enhanced_media_scraper.py`**
   - Scraper am√©lior√© avec contenu complet
   - Validation de qualit√©
   - Syst√®me de retry
   - Cache intelligent

2. **`backend/app/services/enhanced_media_service.py`**
   - Service pour orchestrer le scraping am√©lior√©
   - Scraping multi-sources
   - Sauvegarde en base de donn√©es

### Fichiers Modifi√©s

1. **`backend/app/api/v1/endpoints/media.py`**
   - Endpoint `/latest` avec d√©clenchement automatique
   - Nouvel endpoint `/trigger-scraping`
   - Retour du contenu complet

---

## üöÄ Utilisation

### D√©clencher le Scraping Manuellement

```bash
# Via API
curl -X POST http://localhost:8001/api/v1/media/trigger-scraping

# Via Python
from app.services.enhanced_media_service import EnhancedMediaService
import asyncio

service = EnhancedMediaService()
result = asyncio.run(service.scrape_all_sources())
print(result)
```

### R√©cup√©rer les Articles

```bash
# R√©cup√©rer les articles (d√©clenche automatiquement le scraping si n√©cessaire)
curl http://localhost:8001/api/v1/media/latest?limit=50&auto_scrape=true

# Sans d√©clenchement automatique
curl http://localhost:8001/api/v1/media/latest?limit=50&auto_scrape=false
```

---

## üìà R√©sultats Attendus

### Qualit√© des Articles

- **Avant** : 50-200 caract√®res (titre + r√©sum√©)
- **Apr√®s** : 300-2000+ caract√®res (contenu complet)

### Taux de Succ√®s

- **Avant** : ~60-70% (beaucoup d'articles sans contenu)
- **Apr√®s** : ~85-95% (validation de qualit√© + retry)

### Fra√Æcheur

- **Avant** : Articles peuvent √™tre anciens (pas de filtre)
- **Apr√®s** : Seulement articles < 7 jours

### Exp√©rience Utilisateur

- **Avant** : Articles parfois incomplets, pas de scraping automatique
- **Apr√®s** : Articles complets, scraping automatique en arri√®re-plan, contenu de qualit√©

---

## üîÑ Prochaines √âtapes

### Am√©liorations Futures (Optionnelles)

1. **D√©tection de doublons am√©lior√©e** : Utiliser la similarit√© s√©mantique (embedding)
2. **Cache distribu√©** : Utiliser Redis pour le cache partag√©
3. **Scraping parall√®le** : Scraper plusieurs articles en parall√®le (avec limite)
4. **Analyse de sentiment** : Int√©grer l'analyse LLM du sentiment directement dans le scraper
5. **Notifications** : Notifier l'utilisateur quand de nouveaux articles sont disponibles

---

## ‚úÖ Tests

### Tester le Scraper

```python
from app.pipelines.ingestion.enhanced_media_scraper import EnhancedMediaScraper

scraper = EnhancedMediaScraper()
article = scraper.scrape_article(
    "https://medias24.com/economie/article-exemple",
    "medias24"
)
print(f"Titre: {article.title}")
print(f"Contenu: {article.content[:200]}...")
print(f"Qualit√©: {article.quality_score}")
```

### Tester le Service

```python
from app.services.enhanced_media_service import EnhancedMediaService
import asyncio

service = EnhancedMediaService()
result = asyncio.run(service.scrape_all_sources())
print(result)
```

---

## üéâ Conclusion

Le syst√®me de scraping a √©t√© consid√©rablement am√©lior√© :

- ‚úÖ **Contenu complet** : Chaque article est scrap√© individuellement avec son contenu complet
- ‚úÖ **Qualit√© valid√©e** : Score de qualit√© robuste (0-1)
- ‚úÖ **Retry intelligent** : Backoff exponentiel pour les √©checs
- ‚úÖ **Cache intelligent** : √âvite de re-scraper les m√™mes articles
- ‚úÖ **Scraping automatique** : D√©clenchement automatique quand l'utilisateur acc√®de
- ‚úÖ **Exp√©rience utilisateur** : Articles de qualit√© disponibles imm√©diatement

**La plateforme est maintenant pr√™te √† offrir des articles de qualit√© en temps r√©el !** üöÄ






